# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Architecture Overview

Synevyr is a full-stack web application with a Flask backend and Next.js frontend. The application handles customer data analysis with real-time processing capabilities using Celery workers.

**Core Components:**
- **Backend**: Flask application with SQLAlchemy, Celery for background tasks, MySQL database
- **Frontend**: Next.js 15 with React 19, TypeScript, TailwindCSS
- **Database**: MySQL with custom data models for customer analysis, orders, and leads
- **Task Queue**: Celery with Redis for background data processing
- **Data Generation**: Synthetic data generators for development/testing

## Development Commands

### Initial Setup
```bash
# First-time setup (creates venv, installs deps, seeds database)
python3 run_me_first.py

# Restart all services after initial setup
python3 autostart.py
```

### Backend Development
```bash
cd backend
source ../.synevyr_venv/bin/activate
python run.py  # Starts Flask server on port 2001
```

### Frontend Development  
```bash
cd frontend
npm run dev    # Starts Next.js dev server on port 2000
npm run build  # Build for production
npm run lint   # Lint the frontend code
```

### Background Services
```bash
# Start Celery worker (from backend directory)
source ../.synevyr_venv/bin/activate
celery -A celery_app.celery worker --loglevel=INFO -E

# Start Celery beat scheduler (from backend directory)  
source ../.synevyr_venv/bin/activate
celery -A celery_app.celery beat --loglevel=INFO
```

### Testing
```bash
# Backend testing (from backend directory)
source ../.synevyr_venv/bin/activate
pytest

# Frontend testing
cd frontend  
npm test
```

## Project Structure

### Backend Architecture (`backend/`)
- **`app/__init__.py`** - Flask app factory with CORS, middleware, blueprint registration
- **`app/core/config.py`** - Configuration classes for development/production environments
- **`app/endpoints/`** - API route blueprints (auth, data_sources, tasks, general, open_data)
- **`app/models/`** - SQLAlchemy database models
- **`app/tasks/`** - Celery background task definitions
- **`app/utils/`** - Utility functions including database initialization and logging
- **`run.py`** - Main Flask application entry point
- **`celery_app.py`** - Celery worker/beat configuration

### Frontend Architecture (`frontend/`)
- Next.js 15 application with App Router
- React 19 with TypeScript
- TailwindCSS for styling
- Real-time communication via Socket.IO
- Chart.js for data visualization

### Database Schema

**User Platform Data Pipeline (Production Architecture):**
- `data_sources` - User's configured API endpoints and connection details
- `user_dataset_raw` - Raw JSON data ingested from user's connected API sources
- `leads_clean`, `customers_clean`, `orders_clean` - Clean staging tables from ETL transform
- `source_metrics_daily` - Processed analytics and metrics for dashboard display
- `analytics_etl_state` - ETL cursor tracking for incremental processing

**IMPORTANT: The platform NEVER accesses demo tables directly. All data must flow through APIs → user_dataset_raw → ETL pipeline.**

**Demo/Development Tables (COMPLETELY ISOLATED from platform):**
- `crm_customers` - Sample CRM customer data with proper customer_id relationships  
- `wc_orders` - Sample WooCommerce orders linked to customers via customer_id FK
- `leads` - Sample advertising lead data (isolated, no customer linking required)
- `customer_analysis` - Legacy unified customer analysis
- `cx_stats` - Legacy customer statistics

**Demo Data Usage:**
- These tables exist ONLY for testing platform functionality via API simulation
- Demo data has proper cardinality: crm_customers.id ← wc_orders.customer_id (1:many)
- Platform accesses demo data ONLY through API endpoints (e.g., `/api/customers`, `/api/orders`)
- The platform ETL processes API responses, not direct table access

### Data Generators (`generators/`)
Synthetic data generation for development:
- `generator.py` - Main seeding orchestrator
- `make_me_a_person.py` - CRM customer data generation
- `make_me_wc_orders.py` - Order data generation  
- `make_me_meta_leads.py` - Lead data generation
- `location_repo.py` - Geographic data repository

## Environment Configuration

### Required Environment Files
- **`keys.env`** - Main environment configuration (auto-generated by run_me_first.py)
- **`frontend/.env.local`** - Frontend development environment
- **`frontend/.env.production`** - Frontend production environment

### Key Environment Variables
- `FLASK_ENV` - development/production
- `DATABASE` - Database name (db_synevyr)
- `DEV_DB_*` - Local MySQL connection parameters
- `FLASK_SESSION_KEY` - Session encryption key
- `NEXT_PUBLIC_API_BASE_URL` - Frontend API endpoint
- `NEXT_PUBLIC_SOCKET_URL` - WebSocket connection URL

## Development Workflow

1. Use `python3 run_me_first.py` for first-time setup
2. Use `python3 autostart.py` for subsequent development sessions (macOS only)
3. Both backend (port 2001) and frontend (port 2000) run concurrently
4. Celery workers handle background data processing tasks
5. Database seeding creates realistic synthetic data for development

## Data Processing Pipeline

**User Analytics Platform (Production Flow):**
1. **Connect Data Sources**: Users configure API endpoints in `/dashboard/connect-data`
2. **Ingest Data**: `refresh_data_sources.py` fetches data → `user_dataset_raw` table  
3. **Process Data**: `clean_user_data.py` processes raw data → `source_metrics_daily` table
4. **Display Metrics**: Dashboard shows analytics from `source_metrics_daily` filtered by user_id

**Key Endpoints:**
- `POST /tasks/run/update-data-sources` - Fetch data from connected API sources
- `POST /tasks/run/build-source-metrics` - Process raw data into dashboard metrics  
- `POST /tasks/run/ingest-and-clean` - Complete workflow (ingest then process)
- `GET /analytics/source-metrics` - Dashboard metrics API

## Prerequisites

- Python 3.x with venv support
- Node.js and npm
- MySQL server (configured with root password "Synevyr_SQL_PWD")  
- Redis server (for Celery task queue)
- macOS (for automated terminal management via autostart.py)